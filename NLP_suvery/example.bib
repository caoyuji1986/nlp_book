%              required fields:                  optional fields
% article:     author, title, journal, year      volume, number, pages, month, note
% book:        author,
% journal: 
% unpublished: 
% techreport:
% phdthesis:
% 
@misc{kim2014convolutional,
	title={Convolutional Neural Networks for Sentence Classification},
	author={Yoon Kim},
	year={2014},
	eprint={1408.5882},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{bengio2003a,
	title={A neural probabilistic language model},
	author={Bengio, Yoshua and Ducharme, Rejean and Vincent, Pascal and Janvin, Christian},
	journal={Journal of Machine Learning Research},
	volume={3},
	number={6},
	pages={1137--1155},
	year={2003}}

@article{blei2003latent,
	title={Latent dirichlet allocation},
	author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	journal={Journal of Machine Learning Research},
	volume={3},
	pages={993--1022},
	year={2003}}
@article{deerwester1990indexing,
	title={Indexing by Latent Semantic Analysis},
	author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard A},
	journal={Journal of the Association for Information Science and Technology},
	volume={41},
	number={6},
	pages={391--407},
	year={1990}}
@article{akhtar2019textrank,
	title={TextRank enhanced Topic Model for Query focussed Text Summarization},
	author={Akhtar, Nadeem and Beg, M M Sufyan and Javed, Hira},
	pages={1--6},
	year={2019}}



@article{suykens2001support,
	title={Support Vector Machines : a nonlinear modelling and control perspective},
	author={Suykens, Johan A K},
	journal={European Journal of Control},
	volume={7},
	pages={311--327},
	year={2001}}


@article{chen2016xgboost,
	title={XGBoost: A Scalable Tree Boosting System},
	author={Chen, Tianqi and Guestrin, Carlos},
	pages={785--794},
	year={2016}}
@article{sutskever2014sequence,
	title={Sequence to Sequence Learning with Neural Networks},
	author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	journal={arXiv: Computation and Language},
	year={2014}}

@article{mikolov2013efficient,
	title={Efficient Estimation of Word Representations in Vector Space},
	author={Mikolov, Tomas and Chen, Kai and Corrado, Greg S and Dean, Jeffrey},
	year={2013}}


@inproceedings{inproceedings,
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	year = {2004},
	month = {01},
	pages = {137-150},
	title = {MapReduce: Simplified Data Processing on Large Clusters},
	volume = {51},
	journal = {Communications of the ACM},
	doi = {10.1145/1327452.1327492}
}
@misc{bahdanau2014neural,
	title={Neural Machine Translation by Jointly Learning to Align and Translate},
	author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	year={2014},
	eprint={1409.0473},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}


@article{ghemawat2003the,
	title={The Google file system},
	author={Ghemawat, Sanjay and Gobioff, Howard and Leung, Shuntak},
	volume={37},
	number={5},
	pages={29--43},
	year={2003}}


@article{humeau2019poly-encoders,
	title={Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring},
	author={Humeau, Samuel and Shuster, Kurt and Lachaux, Marieanne and Weston, Jason},
	journal={arXiv: Computation and Language},
	year={2019}}
@article{yin2016abcnn,
	title={ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs},
	author={Yin, Wenpeng and Schutze, Hinrich and Xiang, Bing and Zhou, Bowen},
	journal={Transactions of the Association for Computational Linguistics},
	volume={4},
	number={1},
	pages={259--272},
	year={2016}}
@article{chen2017enhanced,
	title={Enhanced LSTM for Natural Language Inference},
	author={Chen, Qian and Zhu, Xiaodan and Ling, Zhenhua and Wei, Si and Jiang, Hui and Inkpen, Diana},
	volume={1},
	pages={1657--1668},
	year={2017}}

@article{devlin2018bert,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author={Devlin, Jacob and Chang, Mingwei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv: Computation and Language},
	year={2018}}
@misc{huang2015bidirectional,
	title={Bidirectional LSTM-CRF Models for Sequence Tagging},
	author={Zhiheng Huang and Wei Xu and Kai Yu},
	year={2015},
	eprint={1508.01991},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{strubell2017fast,
	title={Fast and Accurate Entity Recognition with Iterated Dilated Convolutions},
	author={Strubell, Emma and Verga, Patrick and Belanger, David and Mccallum, Andrew},
	pages={2670--2680},
	year={2017}}
@article{vaswani2017attention,
	title={Attention is All you Need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	pages={5998--6008},
	year={2017}}


@article{radford2019language,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2019}
}

@article{mikolov2010recurrent,
	title={Recurrent neural network based language model},
	author={Mikolov, Tomas and Karafiat, Martin and Burget, Lukas and Cernock√Ω, Jan and Khudanpur, Sanjeev},
	pages={1045--1048},
	year={2010}}


@article{dauphin2016language,
	title={Language Modeling with Gated Convolutional Networks},
	author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
	journal={arXiv: Computation and Language},
	year={2016}}
@article{wang2018glue,
	title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
	journal={arXiv: Computation and Language},
	year={2018}}
@article{1997Long,
	title={Long Short-Term Memory},
	author={ Hochreiter, S  and  Schmidhuber, J },
	journal={Neural Computation},
	volume={9},
	number={8},
	pages={1735-1780},
	year={1997},
}

@inproceedings{2018Slot,
	title={Slot-Gated Modeling for Joint Slot Filling and Intent Prediction},
	author={ Goo, Chih Wen  and  Gao, Guang  and  Hsu, Yun Kai  and  Huo, Chih Li  and  Chen, Yun Nung },
	booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	year={2018},
}
@article{2017Fixing,
	title={Fixing Weight Decay Regularization in Adam},
	author={ Loshchilov, Ilya  and  Hutter, Frank },
	year={2017},
}
@inproceedings{Pennington2014Glove,
	title={Glove: Global Vectors for Word Representation},
	author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	booktitle={Conference on Empirical Methods in Natural Language Processing},
	year={2014},
}
@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, Jurgen},
	journal={Neural Computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997}}
@article{lai2015recurrent,
	title={Recurrent convolutional neural networks for text classification},
	author={Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	pages={2267--2273},
	year={2015}}


@article{joulin2017bag,
	title={Bag of Tricks for Efficient Text Classification},
	author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	volume={2},
	pages={427--431},
	year={2017}}
@article{yang2016hierarchical,
	title={Hierarchical Attention Networks for Document Classification},
	author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alexander J and Hovy, Eduard},
	pages={1480--1489},
	year={2016}}

@article{zhang2015character-level,
	title={Character-level convolutional networks for text classification},
	author={Zhang, Xiang and Zhao, Junbo and Lecun, Yann},
	pages={649--657},
	year={2015}}

@article{Talman2019Sentence,
	title={Sentence embeddings in NLI with iterative refinement encoders},
	author={Talman, Aarne and Yli-Jyra, Anssi and Tiedemann, Joerg},
	journal={Natural Language Engineering},
	volume={25},
	number={PT.4},
	pages={467-482},
	year={2019},
}

@inproceedings{2016Pairwise,
	title={Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement},
	author={ He, Hua  and  Lin, Jimmy J },
	booktitle={Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	year={2016},
}
@inproceedings{peters-etal-2018-deep,
	title = "Deep Contextualized Word Representations",
	author = "Peters, Matthew  and
	Neumann, Mark  and
	Iyyer, Mohit  and
	Gardner, Matt  and
	Clark, Christopher  and
	Lee, Kenton  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N18-1202",
	doi = "10.18653/v1/N18-1202",
	pages = "2227--2237",
	abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@article{Moody2016Mixing,
	title={Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec},
	author={Moody, Christopher E},
	year={2016},
}
@article{2019Hierarchically,
	title={Hierarchically-Refined Label Attention Network for Sequence Labeling},
	author={ Cui, Leyang  and  Zhang, Yue },
	year={2019},
}
@article{Liu2016Attention,
	title={Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling},
	author={Liu, Bing and Lane, Ian},
	year={2016},
}
@article{2016Attention,
	title={Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling},
	author={ Liu, Bing  and  Lane, Ian },
	year={2016},
}

@article{Luong2015Effective,
	title={Effective Approaches to Attention-based Neural Machine Translation},
	author={Luong, Minh Thang and Pham, Hieu and Manning, Christopher D},
	journal={Computer Science},
	year={2015},
}
@misc{seo2016bidirectional,
	title={Bidirectional Attention Flow for Machine Comprehension},
	author={Minjoon Seo and Aniruddha Kembhavi and Ali Farhadi and Hannaneh Hajishirzi},
	year={2016},
	eprint={1611.01603},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{Lan2019ALBERT,
	title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	year={2019},
}
@article{Lan2019ALBERT,
	title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	year={2019},
}
@Book{ID_lihang,
author = {ÊùéËà™},
title = {ÁªüËÆ°Êú∫Âô®Â≠¶‰π†},
publisher = {Ê∏ÖÂçéÂ§ßÂ≠¶Âá∫ÁâàÁ§æ},
year = {1997 },
}
@Book{ID_zhouzhihua,
	author = {Âë®ÂøóÂçé},
	title = {Êú∫Âô®Â≠¶‰π†},
	publisher = {Ê∏ÖÂçéÂ§ßÂ≠¶Âá∫ÁâàÁ§æ},
	year = {2018},
}
@article{2018A,
	title={A Survey on Deep Learning for Named Entity Recognition},
	author={ Li, Jing  and  Sun, Aixin  and  Han, Jianglei  and  Li, Chenliang },
	year={2018},
}
@inproceedings{2018Deep,
	title={Deep Learning Architectures for Named Entity Recognition: A Survey},
	author={ Thomas, Anu  and  Sangeetha, S. },
	booktitle={3rd International Conference on Advanced Computing and Intelligent Engineering},
	year={2018},
}
@unknown{unknown,
	author = {Qiu, Boyu and Chen, Xu and Xu, Jungang and Sun, Yingfei},
	year = {2019},
	month = {06},
	pages = {},
	title = {A Survey on Neural Machine Reading Comprehension}
}
@article{Baradaran2020A,
	title={A Survey on Machine Reading Comprehension Systems},
	author={Baradaran, Razieh and Ghiasi, Razieh and Amirkhani, Hossein},
	year={2020},
}
@article{2016Bidirectional,
	title={Bidirectional Attention Flow for Machine Comprehension},
	author={ Seo, Minjoon  and  Kembhavi, Aniruddha  and  Farhadi, Ali  and  Hajishirzi, Hannaneh },
	year={2016},
}
@article{2016Machine,
	title={Machine Comprehension Using Match-LSTM and Answer Pointer},
	author={ Wang, Shuohang  and  Jiang, Jing },
	year={2016},
}
@article{2018QANet,
	title={QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension},
	author={ Yu, Adams Wei  and  Dohan, David  and  Luong, Minh Thang  and  Zhao, Rui  and  Chen, Kai  and  Norouzi, Mohammad  and  Le, Quoc V },
	year={2018},
}
@article{2016ReasoNet,
	title={ReasoNet: Learning to Stop Reading in Machine Comprehension},
	author={ Shen, Yelong  and  Huang, Po Sen  and  Gao, Jianfeng  and  Chen, Weizhu },
	year={2016},
}
@article{2017MEMEN,
	title={MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension},
	author={ Pan, Boyuan  and  Li, Hao  and  Zhao, Zhou  and  Cao, Bin  and  Cai, Deng  and  He, Xiaofei },
	year={2017},
}
@article{2015Teaching,
	title={Teaching Machines to Read and Comprehend},
	author={ Hermann, Karl Moritz  and Koisk, Tom√° and  Grefenstette, Edward  and  Espeholt, Lasse  and  Kay, Will  and  Suleyman, Mustafa  and  Blunsom, Phil },
	year={2015},
}
@inproceedings{2016Text,
	title={Text Understanding with the Attention Sum Reader Network},
	author={ Kadlec, Rudolf  and  Schmid, Martin  and  Bajgar, Ondej  and  Kleindienst, Jan },
	booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	year={2016},
}
@inproceedings{2016Text,
	title={Text Understanding with the Attention Sum Reader Network},
	author={ Kadlec, Rudolf  and  Schmid, Martin  and  Bajgar, Ondej  and  Kleindienst, Jan },
	booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	year={2016},
}
@article{2017RACE,
	title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
	author={ Lai, Guokun  and  Xie, Qizhe  and  Liu, Hanxiao  and  Yang, Yiming  and  Hovy, Eduard },
	year={2017},
}
@article{2016Iterative,
	title={Iterative Alternating Neural Attention for Machine Reading},
	author={ Sordoni, Alessandro  and  Bachman, Philip  and  Trischler, Adam  and  Bengio, Yoshua },
	year={2016},
}
@article{2016Dynamic,
	title={Dynamic Coattention Networks For Question Answering},
	author={ Xiong, Caiming  and  Zhong, Victor  and  Socher, Richard },
	year={2016},
}
@book{Joachims2009Learning,
	title={Learning to rank for information retrieval (LR4IR 2007).},
	author={Joachims, Thorsten and Li, Hang and Liu, Tie Yan and Zhai, Chengxiang},
	publisher={Now Publishers,},
	year={2009},
}

@article{papineni2002bleu,
	title={Bleu: a Method for Automatic Evaluation of Machine Translation},
	author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Weijing},
	pages={311--318},
	year={2002}}
@misc{yang2019xlnet,
	title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
	year={2019},
	eprint={1906.08237},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{dai2019transformerxl,
	title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
	author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
	year={2019},
	eprint={1901.02860},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{m2019spanbert,
	title={SpanBERT: Improving Pre-training by Representing and Predicting Spans},
	author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
	year={2019},
	eprint={1907.10529},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}



@article{gehring2017convolutional,
	title={Convolutional Sequence to Sequence Learning},
	author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
	journal={arXiv: Computation and Language},
	year={2017}}

@article{ghemawat2003the,
	title={The Google file system},
	author={Ghemawat, Sanjay and Gobioff, Howard and Leung, Shuntak},
	volume={37},
	number={5},
	pages={29--43},
	year={2003}}


@article{maitrey2015mapreduce,
	title={MapReduce: Simplified Data Analysis of Big Data},
	author={Maitrey, Seema and Jha, C K},
	journal={Procedia Computer Science},
	volume={57},
	pages={563--571},
	year={2015}}


@article{chang2008bigtable,
	title={Bigtable: A Distributed Storage System for Structured Data},
	author={Chang, Fay W and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Michael and Chandra, Tushar Deepak and Fikes, Andrew and Gruber, Robert},
	journal={ACM Transactions on Computer Systems},
	volume={26},
	number={2},
	pages={4--26},
	year={2008}}


@article{shi2010list-wise,
	title={List-wise learning to rank with matrix factorization for collaborative filtering},
	author={Shi, Yue and Larson, Martha and Hanjalic, Alan},
	pages={269--272},
	year={2010}}



@article{shen2014a,
	title={A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval},
	author={Shen, Yelong and He, Xiaodong and Gao, Jianfeng and Deng, Li and Mesnil, Gregoire},
	pages={101--110},
	year={2014}}

@MISC{Kiefel_underreview,
	author = {Martin Kiefel and Varun Jampani and Peter V. Gehler},
	title = {Under review as a workshop contribution at ICLR 2015 PERMUTOHEDRAL LATTICE CNNS},
	year = {}
}



@article{chang2020pre-training,
	title={Pre-training Tasks for Embedding-based Large-scale Retrieval.},
	author={Chang, Weicheng and Yu, Felix X and Chang, Yinwen and Yang, Yiming and Kumar, Sanjiv},
	journal={arXiv: Learning},
	year={2020}}

@misc{li2018survey,
	title={A Survey on Deep Learning for Named Entity Recognition},
	author={Jing Li and Aixin Sun and Jianglei Han and Chenliang Li},
	year={2018},
	eprint={1812.09449},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}


@article{joachims2002optimizing,
	title={Optimizing search engines using clickthrough data},
	author={Joachims, Thorsten},
	pages={133--142},
	year={2002}}



@article{kingma2014adam,
	title={Adam: A Method for Stochastic Optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv: Learning},
	year={2014}}
@book{10.5555/3086952,
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	title = {Deep Learning},
	year = {2016},
	isbn = {0262035618},
	publisher = {The MIT Press},
	abstract = {"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.}
}
@article{chang2008bigtable:,
	title={Bigtable: A Distributed Storage System for Structured Data},
	author={Chang, Fay W and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Michael and Chandra, Tushar Deepak and Fikes, Andrew and Gruber, Robert},
	journal={ACM Transactions on Computer Systems},
	volume={26},
	number={2},
	pages={4--26},
	year={2008}}


@article{dai2019transformer-xl,
	title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.},
	author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc V and Salakhutdinov, Ruslan},
	journal={arXiv: Learning},
	year={2019}}

@article{2017Focal,
	title={Focal Loss for Dense Object Detection},
	author={Lin, Tsung Yi and  Goyal, Priya  and  Girshick, Ross  and  He, Kaiming  and Doll√°r, Piotr},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={PP},
	number={99},
	pages={2999-3007},
	year={2017},
}
@book{IDPatternrecognition,
	author = {Bernd Radig},
	title = {Ê®°ÂºèËØÜÂà´ Patternrecognition},
	date = {2001-12},
}
@book{IDMTBook,
	author = {ËÇñÊ°ê (Tong Xiao) Êú±ÈùñÊ≥¢ (Jingbo Zhu)},
	title = {Êú∫Âô®ÁøªËØëÔºöÁªüËÆ°Âª∫Ê®°‰∏éÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ï},
	OPTurl = {https://github.com/NiuTrans/MTBook},
}
@book{IDCNLP,
	author = {ÁéãÊôìÈæô},
	title = {ËÆ°ÁÆóÊú∫Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ},
	date = {2005-4},
	OPTpublisher = {Ê∏ÖÂçéÂ§ßÂ≠¶Âá∫ÁâàÁ§æ},
}
@book{IDCNLP2,
	author = {ÁΩóÂàö,Âº†Â≠êÂÆ™},
	title = {Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂéüÁêÜ‰∏éÊäÄÊúØÂÆûÁé∞},
	date = {2016-05},
	OPTpublisher = {ÁîµÂ≠êÂ∑•‰∏öÂá∫ÁâàÁ§æ},
}
@article{Lecun2015Deep,
	title={Deep Learning},
	author={Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	journal={Nature},
	volume={521},
	number={7553},
	pages={436-444},
	year={2015},
}
@article{Hinton2014A,
	title={A Fast Learning Algorithm for Deep Belief Nets},
	author={Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	journal={Neural Computation},
	volume={18},
	number={7},
	pages={1527-1554},
	year={2014},
}
@article{Rumelhart1986Learning,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	journal={Nature},
	volume={323},
	number={6088},
	pages={533-536},
	year={1986},
}
@article{Krizhevsky2012ImageNet,
	title={ImageNet Classification with Deep Convolutional Neural Networks},
	author={Krizhevsky, Alex and Sutskever, I. and Hinton, G.},
	year={2012},
}
@article{2019RoBERTa,
	title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	author={ Liu, Yinhan  and  Ott, Myle  and  Goyal, Naman  and  Du, Jingfei  and  Joshi, Mandar  and  Chen, Danqi  and  Levy, Omer  and  Lewis, Mike  and  Zettlemoyer, Luke  and  Stoyanov, Veselin },
	year={2019},
}
@article{2020ERNIE,
	title={ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding},
	author={ Sun, Yu  and  Wang, Shuohuan  and  Li, Yukun  and  Feng, Shikun  and  Wang, Haifeng },
	journal={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={34},
	number={5},
	pages={8968-8975},
	year={2020},
}
@article{2017A,
	title={A Survey on Dialogue Systems: Recent Advances and New Frontiers},
	author={ Chen, Hongshen  and  Liu, Xiaorui  and  Yin, Dawei  and  Tang, Jiliang },
	journal={Acm Sigkdd Explorations Newsletter},
	volume={19},
	number={2},
	pages={25-35},
	year={2017},
}
@article{2018Investigating,
	title={Investigating Capsule Networks with Dynamic Routing for Text Classification},
	author={ Zhao, Wei  and  Ye, Jianbo  and  Yang, Min  and  Lei, Zeyang  and  Zhang, Suofei  and  Zhao, Zhou },
	year={2018},
}
@inproceedings{2017Deep1,
	title={Deep Pyramid Convolutional Neural Networks for Text Categorization},
	author={ Johnson, Rie  and  Zhang, Tong },
	booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	year={2017},
}
@article{2003A,
	title={A Survey on Statistical Language Models},
	author={ Yong-Kang, Xing },
	journal={Computer ence},
	year={2003},
}
@inproceedings{2012A,
	title={A Survey of Learning to Rank for Real-Time Twitter Search},
	author={ Cheng, Fuxing  and  Zhang, Xin  and  He, Ben  and  Luo, Tiejian  and  Wang, Wenjie },
	booktitle={Joint International Conference on Pervasive Computing and the Networked World},
	year={2012},
}
@article{2001Greedy,
	title={Greedy Function Approximation: A Gradient Boosting Machine},
	author={ Friedman, Jerome H. },
	journal={Annals of Statistics},
	volume={29},
	number={5},
	pages={1189-1232},
	year={2001},
}
@inproceedings{LightGBM,
	author = {Meng, Qi},
	year = {2018},
	month = {04},
	title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree}
}
@misc{fayek2016,
	title   = "Speech Processing for Machine Learning: Filter banks, Mel-Frequency Cepstral Coefficients (MFCCs) and What's In-Between",
	author  = "Haytham M. Fayek",
	year    = "2016",
	url     = "https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html"
}

@misc{2004Algorithm,
	title={Algorithm for automatic glottal waveform estimation without the reliance on precise glottal closure information},
	author={Moore E. and Clements M. },
	booktitle={IEEE International Conference on Acoustics},
	year={2004}
}

@article{tyagi2003mel-cepstrum,
	title={Mel-cepstrum modulation spectrum (MCMS) features for robust ASR},
	author={Tyagi, Vivek and Mccowan, Iain A and Misra, Hemant and Bourlard, Herve},
	pages={399--404},
	year={2003}}



















